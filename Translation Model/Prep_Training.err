sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=WikiMatrix.en-nl.en-filtered.en --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true
sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : 
trainer_spec {
  input: WikiMatrix.en-nl.en-filtered.en
  input_format: 
  model_prefix: source
  model_type: UNIGRAM
  vocab_size: 50000
  self_test_sample_size: 0
  character_coverage: 0.9995
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 1
  pretokenization_delimiter: 
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  hard_vocab_limit: 0
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: -1
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ⁇ 
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(183) LOG(INFO) Loading corpus: WikiMatrix.en-nl.en-filtered.en
trainer_interface.cc(407) LOG(INFO) Loaded all 792327 sentences
trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(428) LOG(INFO) Normalizing sentences...
trainer_interface.cc(537) LOG(INFO) all chars count=83460582
trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.
trainer_interface.cc(558) LOG(INFO) Alphabet size=230
trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501
trainer_interface.cc(591) LOG(INFO) Done! preprocessed 792327 sentences.
unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=40934221
unigram_model_trainer.cc(274) LOG(INFO) Initialized 885735 seed sentencepieces
trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 792327
trainer_interface.cc(608) LOG(INFO) Done! 799885
unigram_model_trainer.cc(564) LOG(INFO) Using 799885 sentences for EM training
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=311510 obj=12.7719 num_tokens=2082993 num_tokens/piece=6.68676
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=279775 obj=9.95781 num_tokens=2088410 num_tokens/piece=7.46461
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=209800 obj=9.95042 num_tokens=2157253 num_tokens/piece=10.2824
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=209652 obj=9.93859 num_tokens=2161590 num_tokens/piece=10.3104
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=157237 obj=9.99762 num_tokens=2259624 num_tokens/piece=14.3708
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=157230 obj=9.98411 num_tokens=2259752 num_tokens/piece=14.3723
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=117922 obj=10.0623 num_tokens=2366536 num_tokens/piece=20.0687
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=117921 obj=10.0475 num_tokens=2366523 num_tokens/piece=20.0687
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=88440 obj=10.1436 num_tokens=2476533 num_tokens/piece=28.0024
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=88440 obj=10.1284 num_tokens=2476849 num_tokens/piece=28.006
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=66330 obj=10.2405 num_tokens=2586240 num_tokens/piece=38.9905
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66330 obj=10.2211 num_tokens=2586307 num_tokens/piece=38.9915
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=55000 obj=10.304 num_tokens=2656042 num_tokens/piece=48.2917
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=55000 obj=10.2905 num_tokens=2656176 num_tokens/piece=48.2941
trainer_interface.cc(686) LOG(INFO) Saving model: source.model
trainer_interface.cc(698) LOG(INFO) Saving vocabs: source.vocab
sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=WikiMatrix.en-nl.nl-filtered.nl --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true
sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : 
trainer_spec {
  input: WikiMatrix.en-nl.nl-filtered.nl
  input_format: 
  model_prefix: target
  model_type: UNIGRAM
  vocab_size: 50000
  self_test_sample_size: 0
  character_coverage: 0.9995
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 1
  pretokenization_delimiter: 
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  hard_vocab_limit: 0
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: -1
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ⁇ 
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(183) LOG(INFO) Loading corpus: WikiMatrix.en-nl.nl-filtered.nl
trainer_interface.cc(407) LOG(INFO) Loaded all 792327 sentences
trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(428) LOG(INFO) Normalizing sentences...
trainer_interface.cc(537) LOG(INFO) all chars count=88400162
trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.
trainer_interface.cc(558) LOG(INFO) Alphabet size=171
trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503
trainer_interface.cc(591) LOG(INFO) Done! preprocessed 792327 sentences.
unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=43144546
unigram_model_trainer.cc(274) LOG(INFO) Initialized 1000171 seed sentencepieces
trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 792327
trainer_interface.cc(608) LOG(INFO) Done! 964841
unigram_model_trainer.cc(564) LOG(INFO) Using 964841 sentences for EM training
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=341796 obj=13.03 num_tokens=2527675 num_tokens/piece=7.39527
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=293322 obj=10.1172 num_tokens=2531256 num_tokens/piece=8.62962
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=219952 obj=10.1052 num_tokens=2595854 num_tokens/piece=11.8019
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=219688 obj=10.0925 num_tokens=2596521 num_tokens/piece=11.8191
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=164760 obj=10.1605 num_tokens=2704704 num_tokens/piece=16.416
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=164757 obj=10.1448 num_tokens=2704793 num_tokens/piece=16.4169
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=123566 obj=10.2414 num_tokens=2826074 num_tokens/piece=22.871
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=123565 obj=10.2206 num_tokens=2826389 num_tokens/piece=22.8737
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=92672 obj=10.3415 num_tokens=2952456 num_tokens/piece=31.8592
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=92671 obj=10.3236 num_tokens=2952555 num_tokens/piece=31.8606
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=69503 obj=10.4618 num_tokens=3080784 num_tokens/piece=44.3259
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=69503 obj=10.4369 num_tokens=3081303 num_tokens/piece=44.3334
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=55000 obj=10.5687 num_tokens=3186454 num_tokens/piece=57.9355
unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=55000 obj=10.5445 num_tokens=3186565 num_tokens/piece=57.9375
trainer_interface.cc(686) LOG(INFO) Saving model: target.model
trainer_interface.cc(698) LOG(INFO) Saving vocabs: target.vocab
Corpus corpus_1's weight should be given. We default it to 1 for you.
[2023-10-25 13:47:23,722 INFO] Counter vocab from -1 samples.
[2023-10-25 13:47:23,722 INFO] n_sample=-1: Build vocab on full datasets.
[2023-10-25 13:47:25,423 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=13)

[2023-10-25 13:47:25,424 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,425 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,425 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,426 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,426 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=7)

[2023-10-25 13:47:25,426 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=12)

[2023-10-25 13:47:25,426 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=5)

[2023-10-25 13:47:25,426 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=7)

[2023-10-25 13:47:25,426 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=5)

[2023-10-25 13:47:25,427 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=5)

[2023-10-25 13:47:25,427 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=7)

[2023-10-25 13:47:25,427 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,428 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=11)

[2023-10-25 13:47:25,428 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=12)

[2023-10-25 13:47:25,428 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,428 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=10)

[2023-10-25 13:47:25,428 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=13)

[2023-10-25 13:47:25,428 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=12)

[2023-10-25 13:47:25,429 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=11)

[2023-10-25 13:47:25,429 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=12)

[2023-10-25 13:47:25,429 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=11)

[2023-10-25 13:47:25,429 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,430 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=7)

[2023-10-25 13:47:25,430 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=11)

[2023-10-25 13:47:25,431 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,431 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,431 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,431 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,431 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,431 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,431 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=15)

[2023-10-25 13:47:25,432 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,432 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=3)

[2023-10-25 13:47:25,432 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,432 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,432 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=11)

[2023-10-25 13:47:25,433 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=10)

[2023-10-25 13:47:25,433 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=15)

[2023-10-25 13:47:25,433 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=4)

[2023-10-25 13:47:25,433 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=12)

[2023-10-25 13:47:25,433 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,433 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,433 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=10)

[2023-10-25 13:47:25,433 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=13)

[2023-10-25 13:47:25,434 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=5)

[2023-10-25 13:47:25,434 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=13)

[2023-10-25 13:47:25,434 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=10)

[2023-10-25 13:47:25,434 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=12)

[2023-10-25 13:47:25,434 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=12)

[2023-10-25 13:47:25,435 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,435 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=11)

[2023-10-25 13:47:25,435 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=8)

[2023-10-25 13:47:25,435 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=15)

[2023-10-25 13:47:25,435 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=10)

[2023-10-25 13:47:25,436 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=12)

[2023-10-25 13:47:25,436 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,436 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=6)

[2023-10-25 13:47:25,436 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=11)

[2023-10-25 13:47:25,437 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=6)

[2023-10-25 13:47:25,437 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=9)

[2023-10-25 13:47:25,437 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=3)

[2023-10-25 13:47:25,437 INFO] * Transform statistics for corpus_1(1.56%):
			* FilterTooLongStats(filtered=7)

[2023-10-25 13:47:28,287 INFO] Counters src: 55878
[2023-10-25 13:47:28,287 INFO] Counters tgt: 55231
[2023-10-25 13:47:33,677 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2023-10-25 13:47:33,681 INFO] Parsed 2 corpora from -data.
[2023-10-25 13:47:33,686 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2023-10-25 13:47:33,931 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁the', '.', ',', '▁', '▁of', '1']
[2023-10-25 13:47:33,939 INFO] The decoder start token is: <s>
[2023-10-25 13:47:33,946 INFO] Building model...
[2023-10-25 13:47:35,255 INFO] Switching model to float32 for amp/apex_amp
[2023-10-25 13:47:35,264 INFO] Non quantized layer compute is fp16
[2023-10-25 13:47:43,069 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(50000, 512, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-5): 6 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=False)
          (w_2): Linear(in_features=2048, out_features=512, bias=False)
          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(50000, 512, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=False)
          (w_2): Linear(in_features=2048, out_features=512, bias=False)
          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=512, out_features=512, bias=False)
          (linear_values): Linear(in_features=512, out_features=512, bias=False)
          (linear_query): Linear(in_features=512, out_features=512, bias=False)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=512, out_features=512, bias=False)
        )
        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=512, out_features=50000, bias=True)
)
[2023-10-25 13:47:43,100 INFO] encoder: 44487680
[2023-10-25 13:47:43,122 INFO] decoder: 76435280
[2023-10-25 13:47:43,129 INFO] * number of parameters: 120922960
[2023-10-25 13:47:43,139 INFO] Trainable parameters = {'torch.float32': 120922960, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2023-10-25 13:47:43,146 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2023-10-25 13:47:43,153 INFO]  * src vocab size = 50000
[2023-10-25 13:47:43,160 INFO]  * tgt vocab size = 50000
[2023-10-25 13:47:43,176 INFO] Weighted corpora loaded so far:
			* corpus_1: 1
[2023-10-25 13:47:43,183 INFO] Starting training on GPU: [0]
[2023-10-25 13:47:43,189 INFO] Start training loop and validate every 10000 steps...
[2023-10-25 13:47:43,197 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))
[2023-10-25 13:53:37,422 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=434)

[2023-10-25 13:53:37,430 INFO] Weighted corpora loaded so far:
			* corpus_1: 2
[2023-10-25 14:02:17,802 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=666)

[2023-10-25 14:02:17,810 INFO] Weighted corpora loaded so far:
			* corpus_1: 3
[2023-10-25 14:08:15,646 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=441)

[2023-10-25 14:08:15,680 INFO] Weighted corpora loaded so far:
			* corpus_1: 4
[2023-10-25 14:12:58,542 INFO] Step 4000/200000; acc: 38.3; ppl: 210.1; xent: 5.3; lr: 0.00049; sents: 2254985; bsz: 3440/3647/141; 36323/38502 tok/s;   1515 sec;
[2023-10-25 14:17:01,116 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=661)

[2023-10-25 14:17:01,152 INFO] Weighted corpora loaded so far:
			* corpus_1: 5
[2023-10-25 14:19:15,344 INFO] Saving checkpoint models/model.dutch_step_5000.pt
[2023-10-25 14:25:48,457 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=665)

[2023-10-25 14:25:48,462 INFO] Weighted corpora loaded so far:
			* corpus_1: 6
[2023-10-25 14:31:47,975 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=436)

[2023-10-25 14:31:48,020 INFO] Weighted corpora loaded so far:
			* corpus_1: 7
[2023-10-25 14:38:23,375 INFO] Step 8000/200000; acc: 62.1; ppl:  26.5; xent: 3.3; lr: 0.00099; sents: 2252114; bsz: 3440/3645/141; 36094/38249 tok/s;   3040 sec;
[2023-10-25 14:40:26,390 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=661)

[2023-10-25 14:40:26,466 INFO] Weighted corpora loaded so far:
			* corpus_1: 8
[2023-10-25 14:49:01,501 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=671)

[2023-10-25 14:49:01,504 INFO] Weighted corpora loaded so far:
			* corpus_1: 9
[2023-10-25 14:51:11,742 INFO] valid stats calculation
                           took: 30.4781653881073 s.
[2023-10-25 14:51:11,747 INFO] Train perplexity: 57.3211
[2023-10-25 14:51:11,747 INFO] Train accuracy: 53.3316
[2023-10-25 14:51:11,747 INFO] Sentences processed: 5.63619e+06
[2023-10-25 14:51:11,747 INFO] Average bsz: 3440/3646/141
[2023-10-25 14:51:11,747 INFO] Validation perplexity: 24.5383
[2023-10-25 14:51:11,747 INFO] Validation accuracy: 64.0314
[2023-10-25 14:51:11,747 INFO] Model is improving ppl: inf --> 24.5383.
[2023-10-25 14:51:11,747 INFO] Model is improving acc: -inf --> 64.0314.
[2023-10-25 14:51:11,758 INFO] Saving checkpoint models/model.dutch_step_10000.pt
[2023-10-25 14:55:26,538 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=435)

[2023-10-25 14:55:26,542 INFO] Weighted corpora loaded so far:
			* corpus_1: 10
[2023-10-25 15:03:28,756 INFO] Step 12000/200000; acc: 67.2; ppl:  18.3; xent: 2.9; lr: 0.00081; sents: 2256289; bsz: 3438/3643/141; 36541/38726 tok/s;   4546 sec;
[2023-10-25 15:03:58,901 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=655)

[2023-10-25 15:03:58,902 INFO] Weighted corpora loaded so far:
			* corpus_1: 11
[2023-10-25 15:12:30,220 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=678)

[2023-10-25 15:12:30,224 INFO] Weighted corpora loaded so far:
			* corpus_1: 12
[2023-10-25 15:18:21,903 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=433)

[2023-10-25 15:18:21,907 INFO] Weighted corpora loaded so far:
			* corpus_1: 13
[2023-10-25 15:22:10,942 INFO] Saving checkpoint models/model.dutch_step_15000.pt
[2023-10-25 15:27:02,488 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=660)

[2023-10-25 15:27:02,491 INFO] Weighted corpora loaded so far:
			* corpus_1: 14
[2023-10-25 15:28:25,405 INFO] Step 16000/200000; acc: 71.7; ppl:  14.1; xent: 2.6; lr: 0.00070; sents: 2250900; bsz: 3441/3648/141; 36792/39000 tok/s;   6042 sec;
[2023-10-25 15:32:56,195 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=438)

[2023-10-25 15:32:56,199 INFO] Weighted corpora loaded so far:
			* corpus_1: 15
[2023-10-25 15:41:29,944 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=666)

[2023-10-25 15:41:29,949 INFO] Weighted corpora loaded so far:
			* corpus_1: 16
[2023-10-25 15:50:01,228 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=663)

[2023-10-25 15:50:01,233 INFO] Weighted corpora loaded so far:
			* corpus_1: 17
[2023-10-25 15:53:24,039 INFO] Step 20000/200000; acc: 75.0; ppl:  11.9; xent: 2.5; lr: 0.00062; sents: 2257056; bsz: 3439/3644/141; 36719/38909 tok/s;   7541 sec;
[2023-10-25 15:53:25,265 INFO] * Transform statistics for valid(100.00%):
			* FilterTooLongStats(filtered=41)

[2023-10-25 15:53:58,705 INFO] valid stats calculation
                           took: 34.66059970855713 s.
[2023-10-25 15:53:58,707 INFO] Train perplexity: 27.958
[2023-10-25 15:53:58,707 INFO] Train accuracy: 62.8496
[2023-10-25 15:53:58,707 INFO] Sentences processed: 1.12713e+07
[2023-10-25 15:53:58,707 INFO] Average bsz: 3440/3645/141
[2023-10-25 15:53:58,707 INFO] Validation perplexity: 26.0797
[2023-10-25 15:53:58,707 INFO] Validation accuracy: 64.6421
[2023-10-25 15:53:58,708 INFO] Stalled patience: 3/4
[2023-10-25 15:53:58,725 INFO] Saving checkpoint models/model.dutch_step_20000.pt
[2023-10-25 15:56:40,806 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=445)

[2023-10-25 15:56:40,826 INFO] Weighted corpora loaded so far:
			* corpus_1: 18
[2023-10-25 16:05:30,804 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=657)

[2023-10-25 16:05:30,808 INFO] Weighted corpora loaded so far:
			* corpus_1: 19
[2023-10-25 16:14:22,698 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=669)

[2023-10-25 16:14:22,701 INFO] Weighted corpora loaded so far:
			* corpus_1: 20
[2023-10-25 16:19:21,580 INFO] Step 24000/200000; acc: 77.6; ppl:  10.5; xent: 2.4; lr: 0.00057; sents: 2254235; bsz: 3441/3644/141; 35353/37437 tok/s;   9098 sec;
[2023-10-25 16:20:27,591 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=434)

[2023-10-25 16:20:27,591 INFO] Weighted corpora loaded so far:
			* corpus_1: 21
[2023-10-25 16:25:45,199 INFO] Saving checkpoint models/model.dutch_step_25000.pt
[2023-10-25 16:29:23,975 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=664)

[2023-10-25 16:29:25,362 INFO] Weighted corpora loaded so far:
			* corpus_1: 22
[2023-10-25 16:38:23,917 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=674)

[2023-10-25 16:38:23,921 INFO] Weighted corpora loaded so far:
			* corpus_1: 23
[2023-10-25 16:44:46,220 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=432)

[2023-10-25 16:44:46,224 INFO] Weighted corpora loaded so far:
			* corpus_1: 24
[2023-10-25 16:45:44,202 INFO] Step 28000/200000; acc: 79.6; ppl:   9.6; xent: 2.3; lr: 0.00053; sents: 2254050; bsz: 3437/3647/141; 34752/36866 tok/s;  10681 sec;
[2023-10-25 16:53:46,472 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=656)

[2023-10-25 16:53:46,476 INFO] Weighted corpora loaded so far:
			* corpus_1: 25
[2023-10-25 16:58:38,011 INFO] * Transform statistics for valid(100.00%):
			* FilterTooLongStats(filtered=41)

[2023-10-25 16:59:13,772 INFO] valid stats calculation
                           took: 36.98119616508484 s.
[2023-10-25 16:59:13,774 INFO] Train perplexity: 19.7493
[2023-10-25 16:59:13,775 INFO] Train accuracy: 68.2594
[2023-10-25 16:59:13,775 INFO] Sentences processed: 1.69072e+07
[2023-10-25 16:59:13,775 INFO] Average bsz: 3440/3646/141
[2023-10-25 16:59:13,775 INFO] Validation perplexity: 30.5249
[2023-10-25 16:59:13,775 INFO] Validation accuracy: 64.0972
[2023-10-25 16:59:13,775 INFO] Stalled patience: 2/4
[2023-10-25 16:59:13,792 INFO] Saving checkpoint models/model.dutch_step_30000.pt
[2023-10-25 17:03:29,915 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=677)

[2023-10-25 17:03:30,399 INFO] Weighted corpora loaded so far:
			* corpus_1: 26
[2023-10-25 17:09:43,466 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=435)

[2023-10-25 17:09:43,471 INFO] Weighted corpora loaded so far:
			* corpus_1: 27
[2023-10-25 17:12:10,745 INFO] Step 32000/200000; acc: 81.3; ppl:   8.9; xent: 2.2; lr: 0.00049; sents: 2253868; bsz: 3440/3645/141; 34688/36762 tok/s;  12268 sec;
[2023-10-25 17:18:39,508 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=657)

[2023-10-25 17:18:39,513 INFO] Weighted corpora loaded so far:
			* corpus_1: 28
[2023-10-25 17:24:50,090 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=443)

[2023-10-25 17:24:50,094 INFO] Weighted corpora loaded so far:
			* corpus_1: 29
[2023-10-25 17:31:55,072 INFO] Saving checkpoint models/model.dutch_step_35000.pt
[2023-10-25 17:33:47,827 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=661)

[2023-10-25 17:33:47,830 INFO] Weighted corpora loaded so far:
			* corpus_1: 30
[2023-10-25 17:38:20,224 INFO] Step 36000/200000; acc: 82.8; ppl:   8.4; xent: 2.1; lr: 0.00047; sents: 2252470; bsz: 3440/3646/141; 35067/37168 tok/s;  13837 sec;
[2023-10-25 17:42:49,977 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=667)

[2023-10-25 17:42:49,985 INFO] Weighted corpora loaded so far:
			* corpus_1: 31
[2023-10-25 17:49:00,555 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=440)

[2023-10-25 17:49:00,558 INFO] Weighted corpora loaded so far:
			* corpus_1: 32
[2023-10-25 17:58:00,208 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=657)

[2023-10-25 17:58:00,212 INFO] Weighted corpora loaded so far:
			* corpus_1: 33
[2023-10-25 18:04:30,558 INFO] Step 40000/200000; acc: 84.0; ppl:   8.0; xent: 2.1; lr: 0.00044; sents: 2256409; bsz: 3442/3647/141; 35068/37158 tok/s;  15407 sec;
[2023-10-25 18:04:31,805 INFO] * Transform statistics for valid(100.00%):
			* FilterTooLongStats(filtered=41)

[2023-10-25 18:05:07,380 INFO] valid stats calculation
                           took: 36.80978083610535 s.
[2023-10-25 18:05:07,382 INFO] Train perplexity: 15.9255
[2023-10-25 18:05:07,382 INFO] Train accuracy: 71.9547
[2023-10-25 18:05:07,382 INFO] Sentences processed: 2.25424e+07
[2023-10-25 18:05:07,382 INFO] Average bsz: 3440/3646/141
[2023-10-25 18:05:07,382 INFO] Validation perplexity: 34.3388
[2023-10-25 18:05:07,382 INFO] Validation accuracy: 63.6409
[2023-10-25 18:05:07,382 INFO] Decreasing patience: 3/4
[2023-10-25 18:05:07,396 INFO] Saving checkpoint models/model.dutch_step_40000.pt
[2023-10-25 18:07:37,874 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=671)

[2023-10-25 18:07:38,524 INFO] Weighted corpora loaded so far:
			* corpus_1: 34
[2023-10-25 18:13:48,001 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=435)

[2023-10-25 18:13:48,069 INFO] Weighted corpora loaded so far:
			* corpus_1: 35
[2023-10-25 18:22:43,917 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=657)

[2023-10-25 18:22:43,921 INFO] Weighted corpora loaded so far:
			* corpus_1: 36
[2023-10-25 18:30:50,699 INFO] Step 44000/200000; acc: 85.0; ppl:   7.7; xent: 2.0; lr: 0.00042; sents: 2255872; bsz: 3439/3645/141; 34821/36906 tok/s;  16987 sec;
[2023-10-25 18:31:42,131 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=681)

[2023-10-25 18:31:42,131 INFO] Weighted corpora loaded so far:
			* corpus_1: 37
[2023-10-25 18:37:15,693 INFO] Saving checkpoint models/model.dutch_step_45000.pt
[2023-10-25 18:37:54,043 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=431)

[2023-10-25 18:37:54,043 INFO] Weighted corpora loaded so far:
			* corpus_1: 38
[2023-10-25 18:46:48,330 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=658)

[2023-10-25 18:46:48,425 INFO] Weighted corpora loaded so far:
			* corpus_1: 39
[2023-10-25 18:52:58,689 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=441)

[2023-10-25 18:52:58,692 INFO] Weighted corpora loaded so far:
			* corpus_1: 40
[2023-10-25 18:56:53,908 INFO] Step 48000/200000; acc: 86.0; ppl:   7.5; xent: 2.0; lr: 0.00040; sents: 2255279; bsz: 3440/3645/141; 35210/37309 tok/s;  18551 sec;
[2023-10-25 19:01:53,196 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=666)

[2023-10-25 19:01:53,200 INFO] Weighted corpora loaded so far:
			* corpus_1: 41
[2023-10-25 19:09:44,827 INFO] * Transform statistics for valid(100.00%):
			* FilterTooLongStats(filtered=41)

[2023-10-25 19:10:20,308 INFO] valid stats calculation
                           took: 36.75258255004883 s.
[2023-10-25 19:10:20,311 INFO] Train perplexity: 13.7096
[2023-10-25 19:10:20,311 INFO] Train accuracy: 74.705
[2023-10-25 19:10:20,311 INFO] Sentences processed: 2.81798e+07
[2023-10-25 19:10:20,311 INFO] Average bsz: 3440/3646/141
[2023-10-25 19:10:20,311 INFO] Validation perplexity: 37.3719
[2023-10-25 19:10:20,311 INFO] Validation accuracy: 63.2406
[2023-10-25 19:10:20,311 INFO] Decreasing patience: 2/4
[2023-10-25 19:10:20,324 INFO] Saving checkpoint models/model.dutch_step_50000.pt
[2023-10-25 19:11:28,657 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=666)

[2023-10-25 19:11:28,658 INFO] Weighted corpora loaded so far:
			* corpus_1: 42
[2023-10-25 19:17:38,601 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=441)

[2023-10-25 19:17:38,612 INFO] Weighted corpora loaded so far:
			* corpus_1: 43
[2023-10-25 19:23:12,061 INFO] Step 52000/200000; acc: 86.7; ppl:   7.2; xent: 2.0; lr: 0.00039; sents: 2253164; bsz: 3440/3647/141; 34887/36980 tok/s;  20129 sec;
[2023-10-25 19:26:34,926 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=661)

[2023-10-25 19:26:35,029 INFO] Weighted corpora loaded so far:
			* corpus_1: 44
[2023-10-25 19:35:17,178 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=665)

[2023-10-25 19:35:17,184 INFO] Weighted corpora loaded so far:
			* corpus_1: 45
[2023-10-25 19:41:11,467 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=436)

[2023-10-25 19:41:11,485 INFO] Weighted corpora loaded so far:
			* corpus_1: 46
[2023-10-25 19:42:16,351 INFO] Saving checkpoint models/model.dutch_step_55000.pt
[2023-10-25 19:48:30,176 INFO] Step 56000/200000; acc: 87.5; ppl:   7.1; xent: 2.0; lr: 0.00037; sents: 2253817; bsz: 3439/3644/141; 36244/38406 tok/s;  21647 sec;
[2023-10-25 19:49:53,311 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=661)

[2023-10-25 19:49:53,321 INFO] Weighted corpora loaded so far:
			* corpus_1: 47
[2023-10-25 19:58:29,497 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=671)

[2023-10-25 19:58:29,505 INFO] Weighted corpora loaded so far:
			* corpus_1: 48
[2023-10-25 20:04:25,109 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=435)

[2023-10-25 20:04:25,238 INFO] Weighted corpora loaded so far:
			* corpus_1: 49
[2023-10-25 20:13:02,077 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=655)

[2023-10-25 20:13:02,082 INFO] Weighted corpora loaded so far:
			* corpus_1: 50
[2023-10-25 20:13:33,787 INFO] Step 60000/200000; acc: 88.1; ppl:   6.9; xent: 1.9; lr: 0.00036; sents: 2253800; bsz: 3439/3645/141; 36591/38786 tok/s;  23151 sec;
[2023-10-25 20:13:34,817 INFO] * Transform statistics for valid(100.00%):
			* FilterTooLongStats(filtered=41)

[2023-10-25 20:14:03,948 INFO] valid stats calculation
                           took: 30.159899473190308 s.
[2023-10-25 20:14:03,950 INFO] Train perplexity: 12.2616
[2023-10-25 20:14:03,950 INFO] Train accuracy: 76.8542
[2023-10-25 20:14:03,950 INFO] Sentences processed: 3.38143e+07
[2023-10-25 20:14:03,950 INFO] Average bsz: 3440/3645/141
[2023-10-25 20:14:03,950 INFO] Validation perplexity: 40.489
[2023-10-25 20:14:03,950 INFO] Validation accuracy: 63.0776
[2023-10-25 20:14:03,950 INFO] Decreasing patience: 1/4
[2023-10-25 20:14:03,961 INFO] Saving checkpoint models/model.dutch_step_60000.pt
[2023-10-25 20:22:11,639 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=678)

[2023-10-25 20:22:11,711 INFO] Weighted corpora loaded so far:
			* corpus_1: 51
[2023-10-25 20:28:03,497 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=433)

[2023-10-25 20:28:03,504 INFO] Weighted corpora loaded so far:
			* corpus_1: 52
[2023-10-25 20:36:40,782 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=660)

[2023-10-25 20:36:40,825 INFO] Weighted corpora loaded so far:
			* corpus_1: 53
[2023-10-25 20:38:46,462 INFO] Step 64000/200000; acc: 88.7; ppl:   6.8; xent: 1.9; lr: 0.00035; sents: 2253139; bsz: 3441/3647/141; 36399/38570 tok/s;  24663 sec;
[2023-10-25 20:42:33,182 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=438)

[2023-10-25 20:42:33,186 INFO] Weighted corpora loaded so far:
			* corpus_1: 54
[2023-10-25 20:44:56,368 INFO] Saving checkpoint models/model.dutch_step_65000.pt
[2023-10-25 20:51:11,635 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=666)

[2023-10-25 20:51:11,639 INFO] Weighted corpora loaded so far:
			* corpus_1: 55
[2023-10-25 20:59:48,507 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=664)

[2023-10-25 20:59:48,511 INFO] Weighted corpora loaded so far:
			* corpus_1: 56
[2023-10-25 21:03:48,769 INFO] Step 68000/200000; acc: 89.2; ppl:   6.6; xent: 1.9; lr: 0.00034; sents: 2254917; bsz: 3438/3646/141; 36620/38827 tok/s;  26166 sec;
[2023-10-25 21:05:43,637 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=444)

[2023-10-25 21:05:43,643 INFO] Weighted corpora loaded so far:
			* corpus_1: 57
[2023-10-25 21:14:17,638 INFO] * Transform statistics for corpus_1(100.00%):
			* FilterTooLongStats(filtered=657)

[2023-10-25 21:14:17,643 INFO] Weighted corpora loaded so far:
			* corpus_1: 58
[2023-10-25 21:16:09,434 INFO] * Transform statistics for valid(100.00%):
			* FilterTooLongStats(filtered=41)

[2023-10-25 21:16:38,993 INFO] valid stats calculation
                           took: 30.76693034172058 s.
[2023-10-25 21:16:38,995 INFO] Train perplexity: 11.2384
[2023-10-25 21:16:38,995 INFO] Train accuracy: 78.5949
[2023-10-25 21:16:38,995 INFO] Sentences processed: 3.94525e+07
[2023-10-25 21:16:38,995 INFO] Average bsz: 3440/3646/141
[2023-10-25 21:16:38,995 INFO] Validation perplexity: 43.4502
[2023-10-25 21:16:38,995 INFO] Validation accuracy: 62.8631
[2023-10-25 21:16:38,995 INFO] Decreasing patience: 0/4
[2023-10-25 21:16:38,995 INFO] Training finished after not improving. Early Stop!
[2023-10-25 21:16:38,995 INFO] Best model found at step 10000
[2023-10-25 21:16:38,995 INFO] earlystopper has_stopped!
[2023-10-25 21:16:39,939 INFO] Saving checkpoint models/model.dutch_step_70000.pt
